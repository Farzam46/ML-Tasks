# -*- coding: utf-8 -*-
"""Regression Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NsfH2hO4cvmr7AYvfqu07i5KUSw3GrVI
"""

import pandas as pd
import numpy as np

df=pd.read_csv('/content/student-por.csv')

df.head()

df.isnull().sum()

df.info()

df.describe()

missing_values = df.isnull().sum()
print(missing_values)

import matplotlib.pyplot as plt
import seaborn as sns

sns.histplot(df['age'], kde=True)
plt.show()
sns.boxplot(x='G3', data=df)
plt.show()

sns.countplot(x='sex', data=df, hue='G3')
plt.show()
sns.countplot(x='school', data=df, hue='G3')
plt.show()
print(df.columns)  # Print the column names of your DataFrame to check if 'G3' is present

df['parental_edu_impact'] = df['Fedu'] + df['Medu']
palette = sns.choose_colorbrewer_palette(data_type='sequential')
sns.histplot(df['parental_edu_impact'], palette=palette)
plt.show()

df['dropout_risk'] = df['absences'] / (df['studytime'] + 1)
sns.histplot(df['dropout_risk'], kde=True)
plt.show()

numeric_df = df.select_dtypes(include=np.number)

plt.figure(figsize=(10, 8))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder

X = df.drop('G3', axis=1)
y = df['G3']
# Create a OneHotEncoder object
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')

# Fit the encoder to the categorical columns and transform them
categorical_cols = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']  # List of categorical columns
encoded_data = encoder.fit_transform(X[categorical_cols])

# Create a DataFrame from the encoded data
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))

# Drop the original categorical columns and concatenate the encoded DataFrame
X = X.drop(categorical_cols, axis=1)
X = pd.concat([X, encoded_df], axis=1)

# Now proceed with train_test_split and model fitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {'Linear Regression': LinearRegression(), 'Ridge': Ridge(), 'Lasso': Lasso(), 'Random Forest': RandomForestRegressor()}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"{name} MSE: {mean_squared_error(y_test, y_pred)}")

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder

# Prepare your data
X = df.drop('G3', axis=1)
y = df['G3']

# One-hot encode categorical variables
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
categorical_cols = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob',
                    'reason', 'guardian', 'schoolsup', 'famsup', 'paid',
                    'activities', 'nursery', 'higher', 'internet', 'romantic']
encoded_data = encoder.fit_transform(X[categorical_cols])
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))
X = X.drop(categorical_cols, axis=1)
X = pd.concat([X, encoded_df], axis=1)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Dictionary to store MSE values
mse_values = {}

# Train models and store MSE
models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'Random Forest': RandomForestRegressor()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse_values[name] = mean_squared_error(y_test, y_pred)

# Now plot the MSE values
model_names = list(mse_values.keys())
mse_results = list(mse_values.values())

plt.figure(figsize=(10, 6))
plt.barh(model_names, mse_results, color='skyblue')
plt.xlabel('Mean Squared Error')
plt.title('Model Performance Comparison')
plt.show()