# -*- coding: utf-8 -*-
"""Task5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lshdl1s_QbXsgNpgTVSFzDGhMNw5KxGQ
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""##**question** **no** ***1***"""

df=pd.read_csv('/content/Queries.csv')

df.isnull().sum()
df.info()
df.describe()

"""## **Question** no **2**"""

# Convert the 'CTR' column to string type
df['CTR'] = df['CTR'].astype(str)

                                                              # Now apply the string operations
df['CTR'] = df['CTR'].str.rstrip('%').astype('float') / 100.0

"""## **Question** no **3**:"""

import matplotlib.pyplot as plt
import re

def clean_and_split_query(query):
  # Remove special characters and convert to lowercase
  query = re.sub(r'[^a-zA-Z0-9\s]', '', query).lower()
  # Split into words
  words = query.split()
  return words


query_column_name = 'Top queries'

# Split each query into words and count the frequency of each word
word_counts = {}
for query in df[query_column_name]:
  words = clean_and_split_query(query)
  for word in words:
    if word in word_counts:
      word_counts[word] += 1
    else:
      word_counts[word] = 1


sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)


top_n = 20            # Plot  top 20 words
plt.figure(figsize=(11, 6))
plt.bar([word for word, count in sorted_word_counts[:top_n]], [count for word, count in sorted_word_counts[:top_n]])
plt.xticks(rotation=90)
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.title('Top {} Most Frequent Words in Search Queries'.format(top_n))
plt.show()

"""# **Question** no **4**:"""

import matplotlib.pyplot as plt
# Sort by Clicks and get the top queries
top_queries_by_clicks = df.sort_values(by='Clicks', ascending=False).head(20)

# Sort by Impressions and get the top queries
top_queries_by_impressions = df.sort_values(by='Impressions', ascending=False).head(20)

# Plot for Clicks
plt.figure(figsize=(10, 6))
plt.bar(top_queries_by_clicks['Top queries'], top_queries_by_clicks['Clicks'])
plt.xticks(rotation=90)
plt.xlabel('Query')
plt.ylabel('Clicks')
plt.title('Top 20 Queries by Clicks')
plt.tight_layout()
plt.show()

# Plot for Impressions
plt.figure(figsize=(10, 6))
plt.bar(top_queries_by_impressions['Top queries'], top_queries_by_impressions['Impressions'])
plt.xticks(rotation=90)
plt.xlabel('Query')
plt.ylabel('Impressions')
plt.title('Top 20 Queries by Impressions')
plt.tight_layout()
plt.show()

"""## **Question** no **5**:"""

import matplotlib.pyplot as plt
# Sort by CTR and get the top and bottom queries
top_ctr_queries = df.sort_values(by='CTR', ascending=False).head(10)
bottom_ctr_queries = df.sort_values(by='CTR', ascending=True).head(10)

# Plot for Top CTR Queries
plt.figure(figsize=(10, 6))
plt.bar(top_ctr_queries['Top queries'], top_ctr_queries['CTR'])
plt.xticks(rotation=90)
plt.xlabel('Query')
plt.ylabel('CTR')
plt.title('Top 10 Queries by CTR')
plt.tight_layout()
plt.show()

# Plot for Bottom CTR Queries
plt.figure(figsize=(10, 6))
plt.bar(bottom_ctr_queries['Top queries'], bottom_ctr_queries['CTR'])
plt.xticks(rotation=90)
plt.xlabel('Query')
plt.ylabel('CTR')
plt.title('Bottom 10 Queries by CTR')
plt.tight_layout()
plt.show()

"""## **Question** no **6**:"""

import matplotlib.pyplot as plt
import seaborn as sns # Import seaborn for heatmap plotting

# Convert 'Clicks', 'Impressions', and 'CTR' columns to numeric, handling errors
df['Clicks'] = pd.to_numeric(df['Clicks'], errors='coerce')
df['Impressions'] = pd.to_numeric(df['Impressions'], errors='coerce')
df['CTR'] = pd.to_numeric(df['CTR'], errors='coerce')

# Calculate the correlation matrix, excluding non-numeric columns
correlation_matrix = df.select_dtypes(include=['number']).corr()

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

"""
Observations from the Correlation Matrix:

* Clicks and Impressions have a strong positive correlation (0.91), suggesting that queries with
  higher impressions tend to also have higher clicks. This is intuitive as more impressions
  generally lead to more opportunities for clicks.

* CTR and Clicks show a moderate positive correlation (0.51), indicating that queries with
  higher click-through rates tend to have more clicks. This makes sense as a higher CTR means a
  larger proportion of impressions are converting into clicks.

* There's a weak positive correlation between CTR and Impressions (0.13), suggesting a slight
  tendency for queries with more impressions to have a slightly higher CTR. This could be due to
  more popular queries getting more exposure and thus, potentially, a slightly higher CTR.

* It's important to note that correlation does not imply causation. While these correlations
  highlight potential relationships, further analysis would be needed to establish causal links.
"""

"""## **Question** no **7**:"""

from sklearn.ensemble import IsolationForest

# Select the numerical features for anomaly detection
features = df[['Clicks', 'Impressions', 'CTR']].dropna()

# Create an Isolation Forest model
model = IsolationForest(contamination=0.05)  # Adjust contamination as needed

# Fit the model and predict anomalies
model.fit(features)
predictions = model.predict(features)

# Add the predictions as a new column to the DataFrame
df['Anomaly'] = predictions

# Filter the DataFrame to show anomalies (-1 indicates an anomaly)
anomalies = df[df['Anomaly'] == -1]
print(anomalies)